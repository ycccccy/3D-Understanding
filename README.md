<div align="center">
    <h1 style="display: inline-flex; align-items: center;">
        Awesome LLMs for 3D Scene Understanding
    </h1>
</div>

This document summarizes cutting-edge research on 3D scene understanding and foundation models, covering innovative frameworks, datasets, and benchmarks for enhanced spatial reasoning. This document will be continuously updated.


## ðŸ“– News
**[2026/01/04]** Release 3D Scene understanding and 3D foundation model methods.

## ðŸŒŸ Overview

- [ðŸ“– 3D Scene Understanding](#3d-scene-understanding)
- [ðŸŒŸ 3D Foundation Model](#-3D-Foundation-Model)


## ðŸŒŸ 3D Scene Understanding
| Method (Pub.) | Paper | Input | Framework | Data | Project |
|---------------|-------|-------|-----------|------|---------|
| **SÂ²-MLLM (2025-12)** | [SÂ²-MLLM: Boosting Spatial Reasoning Capability of MLLMs for 3D Visual Grounding with Structural Guidance](https://arxiv.org/pdf/2512.01223  ) | Multi-view RGB / Text Depth / Point map/ Ray| <img width="700" alt="image" src="./Figs/3D_Scene_Understanding/S2-MLLM.png"/> | ScanRefer / Nr3D / Sr3D / Multiscan / ArkiScenes | |
| **SpaceDrive (2025-12)** | [SpaceDrive: Infusing Spatial Awareness into VLM-based Autonomous Driving](https://arxiv.org/abs/2512.10719v1  ) | Multi-view RGB / Depth / 3D position / Text | <img width="700" alt="image" src="./Figs/3D_Scene_Understanding/Spacedrive.png"/> | nuScenes / Bench2Drive | https://zhenghao2519.github.io/SpaceDrive_Page/   |
| **DSR (2025-12)** | [Learning to Reason in 4D: Dynamic Spatial Reasoning for Vision-Language Models](https://arxiv.org/abs/2512.20557  ) | RGB video / Text /Poses / Masks / Agent| <img width="700" alt="image" src="./Figs/3D_Scene_Understanding/DSR.png"/> <img width="700" alt="image" src="./Figs/3D_Scene_Understanding/DSR2.png"/>| DSR-Bench| https://github.com/TencentARC/DSR_Suite   |
| **VLMÂ² (2025-11)** | [Vision-Language Model with Memory for Spatial Reasoning in 3D Found](https://arxiv.org/abs/2511.20644  ) | RGB video / Text | <img width="700" alt="image" src="./Figs/3D_Scene_Understanding/VLMM.png"/>  | VSI-Bench / VSTI-Bench / ScanQA / SQA3D | https://sairlab.org/vlm2/   |
| **SpaceMind (2025-11)** | [SpaceMind: Camera-Guided Modality Fusion for Spatial Reasoning in Vision-Language Models](https://arxiv.org/abs/2511.23075  ) |-- | <img width="700" alt="image" src="./Figs/3D_Scene_Understanding/SpaceMind.png"/> <img width="700" alt="image" src="./Figs/3D_Scene_Understanding/SpaceMind2.png"/> | VSI-Bench / SQA3D / SPBench | |
| **3DThinker (2025-10)** | [Think with 3D: Geometric Imagination Grounded Spatial Reasoning from Limited Views](https://arxiv.org/abs/2510.18632  ) | RGB Images / Text /3D Features| <img width="700" alt="image" src="./Figs/3D_Scene_Understanding/3Dthinker.png"/> <img width="700" alt="image" src="./Figs/3D_Scene_Understanding/3Dthinker2.png"/> | MindCube-Tiny / Ego3D-Bench / VSI-Bench / SPBench / CV-Bench / SPAR-Bench / ViewSpatialBench / MMSI-Bench | https://github.com/zhangquanchen/3DThinker   |
| **SpaceVista-7B (2025-10)** | [SPACEVISTA: CALL-SCALE VISUAL SPATIAL REASONING FROM mm TO km](https://arxiv.org/abs/2510.09606  ) | RGB video / Text | <img width="700" alt="image" src="./Figs/3D_Scene_Understanding/SPACEVISTA.png"/> <img width="700" alt="image" src="./Figs/3D_Scene_Understanding/SPACEVISTA2.png"/> | SpaceVista | https://github.com/PeiwenSun2000/SpaceVista   |
| **SR-3D (2025-09)** | [SR-3D: Spatial Region 3D Aware Vision-Language Model](https://arxiv.org/abs/2509.13317  ) |  | <img width="700" alt="image" src="./Figs/3D_Scene_Understanding/SR3D.png"/> | Scan2Cap / ScanQA / SQA3D / BLINK / COCO2017 / SR-3D-Bench / VSI-Bench | https://sr-3d.github.io   |
| **SD-VLM (2025-09)** | [SD-VLM: Spatial Measuring and Understanding with Depth-Encoded Vision-Language Models](https://arxiv.org/abs/2509.17664  ) | RGB / Depth / Text | <img width="700" alt="image" src="./Figs/3D_Scene_Understanding/SD-VLM.png"/> <img width="700" alt="image" src="./Figs/3D_Scene_Understanding/SD-VLM2.png"/> <img width="700" alt="image" src="./Figs/3D_Scene_Understanding/SD-VLM3.png"/>| MSMU-Bench / Q-Spatial++ / SpatialRGPT-Bench / Whatsup / GQA / TextVQA / VQA-v2 / Vizwiz | https://github.com/cpystan/SD-VLM   |
| **RynnEC (2025-08)** | [RynnEC: Bringing MLLMs into Embodied World](https://arxiv.org/abs/2508.14160  ) | Video / Text / Masks| <img width="700" alt="image" src="./Figs/3D_Scene_Understanding/RynnEC.png"/> <img width="700" alt="image" src="./Figs/3D_Scene_Understanding/RynnEC2.png"/>| RynnEC-Bench | https://github.com/alibaba-damo-academy/RynnEC   |
| **3D-R1 (2025-07)** | [3D-R1: Enhancing Reasoning in 3D VLMs for Unified Scene Understanding](https://arxiv.org/abs/2507.23478  ) | RGB-D / Text / Point map | <img width="700" alt="image" src="./Figs/3D_Scene_Understanding/3D-R1.png"/> <img width="700" alt="image" src="./Figs/3D_Scene_Understanding/3D-R1-2.png"/> | ScanRefer / Nr3D / ScanQA / Cap3D / 3D-LLM / SQA3D | https://aigeeksgroup.github.io/3D-R1/   |
| **RoboRefer (2025-06)** | [RoboRefer: Towards Spatial Referring with Reasoning in Vision-Language Models for Robotics](https://arxiv.org/pdf/2506.04308  ) | RGB-D / Text | <img width="700" alt="image" src="./Figs/3D_Scene_Understanding/RoboRefer.png"/> | CV-Bench / BLINK / RoboSpatial / SAT / EmbSpatial / RoboRefIt / Where2Place  / RoboSpatial / RefCOCO  / RefCOCO+  / RefCOCOg / NVILA-2B  / RoboRefer | https://zhoues.github.io/RoboRefer/   |
| **3DRS (2025-06)** | [MLLMs Need 3D-Aware Representation Supervision for Scene Understanding](https://arxiv.org/abs/2506.01946  ) | Images / Text / 3D Features | <img width="700" alt="image" src="./Figs/3D_Scene_Understanding/3drs.png"/> <img width="700" alt="image" src="./Figs/3D_Scene_Understanding/3drs2.png"/>| Multi3DRef / ScanRefer3D / Scan2Cap / ScanQA / SQA3D | https://visual-ai.github.io/3drs   |
| **Scene-R1 (2025-06)** | [Scene-R1: VIDEO-GROUNDED LARGE LANGUAGE MODELS FOR 3D SCENE REASONING WITHOUT 3D ANNOTATIONS](https://arxiv.org/abs/2506.17545  ) |  | <img width="700" alt="image" src="./Figs/3D_Scene_Understanding/Scene-R1.png"/> | ScanRefer / VSI-Bench | https://scene-r1.github.io   |
| **SpatialLM (2025-06)** | [SpatialLM: Training Large Language Models for Structured Indoor Modeling](https://arxiv.org/abs/2506.07491  ) |  | <img width="700" alt="image" src="./Figs/3D_Scene_Understanding/SPATIALLM.png"/> | ScanNet \ Layout Estimation \ 3D Object Detection | https://manycore-research.github.io/SpatialLM/   |
| **SSR (2025-05)** | [SSR: Enhancing Depth Perception in Vision-Language Models via Rational-e-Guided Spatial Reasoning](https://arxiv.org/abs/2505.12448  ) | RGB / Depth /Text | <img width="700" alt="image" src="./Figs/3D_Scene_Understanding/SSR.png"/> | SpatialBench / SSRBENCH / CV-Bench / VQA  | https://yliu-cs.github.io/SSR   |
| **Spatial-MLLM (2025-05)** | [Spatial-MLLM: Boosting MLLM Capabilities in Visual-based Spatial Intelligence from Pure 2D Observations](https://arxiv.org/abs/2505.23747  ) | RGB (images or video) / Text | <img width="700" alt="image" src="./Figs/3D_Scene_Understanding/Spatial-MLLM.png"/> | Spatial-MLLM-120k / VSI-Bench / SQA3D / SPBench / Q-Spatial | https://diankun-wu.github.io/Spatial-MLLM/   |
| **SAMA (2025-05)** | [SAMA: Towards Multi-Turn Referential Grounded Video Chat with Large Language Models](https://arxiv.org/abs/2505.18812  ) | video / Text / Masks | <img width="700" alt="image" src="./Figs/3D_Scene_Understanding/SAMA.png"/> | SAMA-Bench / RefCOCO / RefCOCO+ / RefCOCOg / GCG  / MeViS / Ref-DAVIS17 / Ref-YTVOS / ReVOS / MME / MMBench  / SEED-Benc / AI2D  / MMStar / SQA / Video-MME | https://github.com/sunye23/SAMA   |
| **ViewSpatial (2025-05)** | [ViewSpatial-Bench: Evaluating Multi-perspective Spatial Localization in Vision-Language Models](https://arxiv.org/abs/2505.21500  ) |  | <img width="700" alt="image" src="./Figs/3D_Scene_Understanding/ViewSpatial.png"/> <img width="700" alt="image" src="./Figs/3D_Scene_Understanding/ViewSpatial2.png"/>  | ViewSpatial-Bench / VSI-Bench / VSI-App | https://github.com/ZJU-REAL/ViewSpatial-Bench   
| **Ross3D (2025-04)** | [ROSS3D: Reconstructive Visual Instruction Tuning with 3D-Awareness](https://arxiv.org/abs/2504.01901 ) |  | <img width="700" alt="image" src="./Figs/3D_Scene_Understanding/Ross3D.png"/> <img width="700" alt="image" src="./Figs/3D_Scene_Understanding/Ross3D2.png"/>  | Multi3DRef / ScanRefer3D / Scan2Cap / ScanQA / SQA3D | https://haochen-wang409.github.io/ross3d   
| **SITE** | [SITE: BRIDGING TEXT AND IMAGE MODALITIES WITH LLMS FOR 3D SCENE UNDERSTANDING](https://openreview.net/pdf/fa6e3cb48582a19b228a5cbc3373874931a9e970.pdf )|  | <img width="700" alt="image" src="./Figs/3D_Scene_Understanding/SITE.png"/>  | Multi3DRef / ScanRefer3D / Scan2Cap / ScanQA / SQA3D / METEOR / ROUGE / BLEU-4  / CIDEr
|

## ðŸŒ™ 3D Foundation Model
| Method (Pub.) | Paper | Framework | Data | Project |
|---------------|-------|-----------|------|---------|
| **Depth Anything 3 (2025-11)** | [Depth Anything 3: Recovering the Visual Space from Any Views](https://arxiv.org/abs/2511.10647  ) | <img width="700" alt="image" src="./Figs/3D_Foundation_Model/DA3.png"/> | HiRoom / ETH3D / DTU / 7Scenes / ScanNet++ | https://depth-anything-3.github.io   |
| **STream3R (2025-08)** | [STream3R: Scalable Sequential 3D Reconstruction with Causal Transformer](https://arxiv.org/abs/2508.10893  ) | <img width="700" alt="image" src="./Figs/3D_Foundation_Model/STREAM3R.png"/> | Sintel  / Bonn / KITTI / NYU-v2 / TUM-dynamics  / ScanNet | https://nirvanalan.github.io/projects/stream3r/   |
| **Uni3R (2025-08)** | [Uni3R: Unposed 3D Reconstruction and Semantic Understanding via Gaussian Splatting](https://arxiv.org/abs/2508.03643  ) | <img width="700" alt="image" src="./Figs/3D_Foundation_Model/Uni3R.png"/> | ScanNet | https://github.com/dptech-corp/Uni-3DAR   |
| **VGGT-Long (2025-07)** | [VGGT-Long: Pushing VGGTâ€™s Limits on Kilometer-scale RGB Sequences](https://arxiv.org/abs/2507.16443  ) | <img width="700" alt="image" src="./Figs/3D_Foundation_Model/VGGT-Long.png"/> | KITTI / Waymo / Virtual KITTI | https://github.com/DengKaiCQ/VGGT-Long   |
| **StreamVGGT (2025-07)** | [Streaming 4D Visual Geometry Transformer](https://arxiv.org/abs/2507.11539  ) | <img width="700" alt="image" src="./Figs/3D_Foundation_Model/StreamVGGT.png"/> | 7-Scenes / NRGBD / KITTI / Sintel / Bonn / NYU-v2 / Sintel| https://wzzheng.net/StreamVGGT/   |
| **Dens3R (2025-07)** | [Dens3R: A Foundation Model for 3D Geometry Prediction](https://arxiv.org/abs/2507.16290  ) | <img width="700" alt="image" src="./Figs/3D_Foundation_Model/Dens3R.png"/> | NYUv2  / ScanNet / IBims-1  / Sintel / DIODE-outdoor / ZEB | https://github.com/G-1nOnly/Dens3R   |
| **VGGT (2025-03)** | [VGGT: Visual Geometry Grounded Transformer](https://arxiv.org/abs/2503.11651  ) | <img width="700" alt="image" src="./Figs/3D_Foundation_Model/VGGT.png"/> | Re10K / CO3Dv2 / RealEstate10K / DTU / ETH3D / ScanNet-1500  / GSO / TAP-Vid | https://github.com/facebookresearch/vggt   |
| **Fast3R (2025-01)** | [Fast3R: Towards 3D Reconstruction of 1000+ Images in One Forward Pass](https://arxiv.org/abs/2501.13928  ) | <img width="700" alt="image" src="./Figs/3D_Foundation_Model/Fast3R.png"/> | CO3D-v2 / RealEstate10K / 7 Scenes / Mip-NeRF 360 | https://github.com/facebookresearch/fast3r   |
| **CUT3R (2025-01)** | [CUT3R: Continuous Updating Transformer for 3D Reconstruction](https://arxiv.org/abs/2501.12387  ) | <img width="700" alt="image" src="./Figs/3D_Foundation_Model/CUT3R.png"/> | Sintel  / Bonn / KITTI / NYU-v2 / TUM-dynamics  / ScanNet / 7 scenes / NRGBD | https://cut3r.github.io/   |
| **DUSt3R (2023-12)** | [DUSt3R: Geometric 3D Vision Made Easy](https://arxiv.org/abs/2312.14132  ) | <img width="700" alt="image" src="./Figs/3D_Foundation_Model/DUSt3R.png"/> | 7Scenes / Cambridge  / KITTI / ScanNet / ETH3D / DTU / T&T | https://github.com/naver/dust3r   |
