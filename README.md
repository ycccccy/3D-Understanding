<div align="center">
    <h1 style="display: inline-flex; align-items: center;">
        Awesome LLMs for 3D Scene Understanding
    </h1>
</div>
This document summarizes cutting-edge research on 3D scene understanding and foundation models, covering innovative frameworks, datasets, and benchmarks for enhanced spatial reasoning. This document will be continuously updated.


## üìñ News
**[2026/01/04]** Release 3D Scene understanding and 3D foundation model methods.

## üåü Overview

- [üìñ 3D Scene Understanding](#-tasks)
- [üåü 3D Foundation Model](#-overview)



| Method | Paper | Input | Abstract | Framework | Data |
|--------|-------|-------|----------|-----------|------|
| 3DRS | [MLLMs Need 3D-Aware Representation Supervision for Scene Understanding](https://arxiv.org/abs/2506.01946) | Images / Text / 3D Features | Recent advances in scene understanding have leveraged multimodal large language models (MLLMs) for 3D reasoning by capitalizing on their strong 2D pretraining. However, the lack of explicit 3D data during MLLM pretraining limits 3D representation capability. In this paper, we investigate the 3D-awareness of MLLMs by evaluating multi-view correspondence and reveal a strong positive correlation between the quality of 3D-aware representation and downstream task performance. Motivated by this, we propose 3DRS, a framework that enhances MLLM 3D Representation learning through Supervision from pretrained 3D foundation models. Our approach aligns MLLM visual features with rich 3D knowledge distilled from 3D models, effectively improving scene understanding. Extensive experiments across multiple benchmarks and MLLMs‚Äîincluding visual grounding, captioning, and question answering‚Äîdemonstrate consistent performance gains. | ![1767167146731.png](media/image1.png) | Multi3DRef / ScanRefer3D / Scan2Cap / ScanQA / SQA3D |
| SSR | [SSR: Enhancing Depth Perception in Vision-Language Models via Rational-e-Guided Spatial Reasoning](https://arxiv.org/abs/2505.12448) | RGB / Depth | Despite impressive advancements in Visual-Language Models (VLMs) for multi-modal tasks, their reliance on RGB inputs limits precise spatial understanding. Existing methods for integrating spatial cues either require specialized sensors or fail to effectively exploit depth information. We propose SSR, a framework that transforms raw depth into structured textual rationales and distills them into compact latent embeddings for plug-and-play VLM integration without retraining. We also introduce the million-scale SSR-CoT dataset and SSRBench benchmark. Experiments show SSR substantially improves depth utilization and spatial reasoning. | ![1767163234095.png](media/image3.png) | SSR-CoT / ScanRefer / Nr3D / Sr3D |
| S¬≤-MLLM | [S¬≤-MLLM: Boosting Spatial Reasoning Capability of MLLMs for 3D Visual Grounding with Structural Guidance](https://arxiv.org/pdf/2512.01223) | Multi-view RGB / Text | 3D Visual Grounding (3DVG) aims to locate objects in 3D scenes from natural language. MLLMs excel at 2D inputs but struggle with 3D spatial structure. S¬≤-MLLM introduces implicit spatial reasoning: it acquires 3D structural understanding during training via a structure-enhanced module with intra-/inter-view attention and multi-level position encoding, eliminating inefficient point-cloud rendering. Experiments on ScanRefer, Nr3D, Sr3D show superior performance, generalization and efficiency. | ![1767164456963.png](media/image4.png) | ScanRefer / Nr3D / Sr3D |
| SpaceDrive | [SpaceDrive: Infusing Spatial Awareness into VLM-based Autonomous Driving](https://arxiv.org/abs/2512.10719v1) | Multi-view RGB / Depth / Ego-state / Text | End-to-end VLM driving methods lack fine-grained 3D spatial understanding. SpaceDrive treats 3D coordinates as explicit positional encodings instead of digit tokens, enabling joint semantic-spatial reasoning. A universal positional encoder maps multi-view depth, ego-states and text into 3D PEs that augment 2D visual tokens and allow direct coordinate regression. Evaluations on nuScenes and Bench2Drive show state-of-the-art open-loop results and 78.02 Driving Score closed-loop. | ![1767165079692.png](media/image5.png) | nuScenes / Bench2Drive |
| 3DThinker | [Think with 3D: Geometric Imagination Grounded Spatial Reasoning from Limited Views](https://arxiv.org/abs/2510.18632) | RGB (single or few views) / Text | Understanding 3D relationships from limited views remains challenging. 3DThinker enables VLMs to perform ‚Äú3D mentaling‚Äù without any 3D input or labeled 3D data. A two-stage training first aligns 3D latents from a VLM to a 3D foundation model (VGGT), then refines reasoning trajectories with outcome signals only. Extensive benchmarks show consistent gains and a new perspective on unifying 3D representations into multimodal reasoning. | ![1767165346786.png](media/image6.png) | VSI-Bench / SQA3D / SPBench / Q-Spatial |
| VLM¬≤ | [Vision-Language Model with Memory for Spatial Reasoning in 3D Found](https://arxiv.org/abs/2511.20644) | RGB video / Text | Current VLMs fall short in long-horizon video spatial reasoning due to semantic-geometric misalignment and lack of persistent 3D memory. VLM¬≤ introduces a dual-memory module: working memory (sliding window) and episodic memory (long-term consolidation) to maintain a view-consistent 3D-aware representation purely from 2D video. It sets state-of-the-art among video-only models on multiple benchmarks. | ![1767166392937.png](media/image8.png) | VSI-Bench / SQA3D / SPBench |
| DSR | [Learning to Reason in 4D: Dynamic Spatial Reasoning for Vision-Language Models](https://arxiv.org/abs/2512.20557) | RGB video / Text | VLMs excel at static scenes but struggle with dynamic spatial reasoning (DSR) about evolving 3D geometry over time due to scarce 4D-aware data. DSR Suite is proposed: an automated pipeline generates million-scale multiple-choice QA pairs from in-the-wild videos with geometric cues (camera pose, point clouds, trajectories, etc.). A lightweight Geometry Selection Module (GSM) injects question-relevant 4D priors into VLM. Qwen2.5-VL-7B + DSR significantly boosts dynamic spatial reasoning while maintaining general video understanding. | ![1767166629215.png](media/image9.png) | DSR-Train / DSR-Bench / Kubric-4D / Something-Something V2 |
| RoboRefer | [RoboRefer: Towards Spatial Referring with Reasoning in Vision-Language Models for Robotics](https://arxiv.org/pdf/2506.04308) | RGB-D / Text | Spatial referring is crucial for embodied robots, yet VLMs cannot accurately understand complex 3D scenes and reason about interaction locations. RoboRefer is a 3D-aware VLM: (1) supervised fine-tuning integrates a disentangled depth encoder for precise spatial understanding; (2) reinforcement fine-tuning with metric-sensitive process rewards enables multi-step spatial reasoning. RefSpatial dataset (20M QA, 31 relations) and RefSpatial-Bench are introduced. SFT achieves 89.6% success; RFT surpasses Gemini-2.5-Pro by 12.4%. | ![1767167344931.png](media/image11.png) | RefSpatial / RefSpatial-Bench |
| RynnEC | [RynnEC: Bringing MLLMs into Embodied World](https://arxiv.org/abs/2508.14160) | Egocentric video / Text | RynnEC is a video MLLM for embodied cognition. A compact region encoder + mask decoder enables flexible region-level video interaction. Despite its small size it achieves SOTA on object property understanding, segmentation and spatial reasoning. An egocentric video pipeline generates embodied cognition data; RynnEC-Bench is proposed for evaluation. Code & model are released. | ![1767167930129.png](media/image12.png) | RynnEC-Bench / Ego4D / Epic-Kitchens |
| Spatial-MLLM | [Spatial-MLLM: Boosting MLLM Capabilities in Visual-based Spatial Intelligence from Pure 2D Observations](https://arxiv.org/abs/2505.23747) | RGB (images or video) / Text | Existing 3D MLLMs rely on extra 3D/2.5D inputs, limiting 2D-only scenarios. Spatial-MLLM unleashes structure priors from a feed-forward visual-geometry foundation model. A dual-encoder extracts semantic (2D) and structure (3D) features; a space-aware frame sampling strategy selects spatially informative frames. Trained on Spatial-MLLM-120k, it achieves SOTA on wide real-world datasets for visual spatial understanding. | ![1767168801510.png](media/image14.png) | Spatial-MLLM-120k / VSI-Bench / SQA3D / SPBench / Q-Spatial |
| SAMA | [SAMA: Towards Multi-Turn Referential Grounded Video Chat with Large Language Models](https://arxiv.org/abs/2505.18812) | RGB video / Text | Fine-grained spatio-temporal understanding in videos requires jointly mastering video referring (semantics) and video grounding (segmentation). SAMA contributes: (1) SAMA-239K dataset for joint learning; (2) SAMA model with spatio-temporal context aggregator + SAM segmentation head; (3) SAMA-Bench (5 067 Q from 522 videos). SAMA sets new SOTA on SAMA-Bench and general grounding benchmarks while maintaining competitive video understanding performance. | ![1767168801510.png](media/image15.png) | SAMA-239K / SAMA-Bench / Ref-Youtube-VOS / Ref-DAVIS17 |
| 3D-R1 | [3D-R1: Enhancing Reasoning in 3D VLMs for Unified Scene Understanding](https://arxiv.org/abs/2507.23478) | RGB-D / Text | Current 3D VLMs struggle with robust reasoning due to limited high-quality spatial data and static viewpoint assumptions. 3D-R1 is a foundation model that first constructs Scene-30K, a synthetic CoT dataset via Gemini-2.5-Pro, then applies RLHF (GRPO) with three rewards (perception, semantic, format) to enhance reasoning. A dynamic view-selection strategy adaptively chooses informative perspectives. 3D-R1 improves average performance by 10% across diverse 3D scene benchmarks. | ![1767191167038.png](media/image17.png) | Scene-30K / ScanQA / SQA3D / ScanRefer / Multi3DRef |
| SD-VLM | [SD-VLM: Spatial Measuring and Understanding with Depth-Encoded Vision-Language Models](https://arxiv.org/abs/2509.17664) | RGB / Depth / Text | VLMs excel at semantic understanding but lag in quantitative 3D spatial reasoning. SD-VLM introduces (1) MSMU dataset: 700K QA pairs, 2.5M physical annotations, 10K CoT; (2) simple depth positional encoding to boost VLM spatial awareness. SD-VLM achieves SOTA on MSMU-Bench and shows strong generalization on Q-Spatial and SpatialRGPT Bench, outperforming GPT-4o and Intern-VL3-78B by large margins. | ![1767191786443.png](media/image19.png) | MSMU / MSMU-Bench / Q-Spatial / SpatialRGPT Bench |
| SpaceMind | [SpaceMind: Camera-Guided Modality Fusion for Spatial Reasoning in Vision-Language Models](https://arxiv.org/abs/2511.23075) | RGB / Camera parameters / Text | VLMs still struggle with distance, size comparison and cross-view consistency. SpaceMind adopts a dual-encoder architecture (InternViT + VGGT) and a lightweight Camera-Guided Modality Fusion module that treats camera representation as an active guiding modality rather than passive metadata. The module applies camera-conditioned biasing and query-independent weighting. SpaceMind sets new SOTA on VSI-Bench, SQA3D and SPBench, demonstrating effective spatially grounded intelligence. | ![1767192138629.png](media/image22.png) | VSI-Bench / SQA3D / SPBench |
| Ross3D | [Ross3D: Reconstructive Visual Instruction Tuning with 3D-Awareness](https://arxiv.org/abs/2504.01901) | Multi-view RGB / Text | Adapting 2D LMMs to 3D scenes usually relies on designing 3D input-level representations. Ross3D proposes reconstructive visual instruction tuning: training objectives are cross-view reconstruction (recover masked views) and global-view reconstruction (recover Bird‚Äôs-Eye-View images), injecting 3D-aware supervision without extra sensors. Semi-supervised experiments show significant potential to leverage large-scale unlabeled 3D vision-only data while achieving SOTA on multiple 3D scene understanding benchmarks. | ![1767192560146.png](media/image24.png) | ScanRefer / ScanQA / SQA3D / Multi3DRef |
| Scene-R1 | [Scene-R1: VIDEO-GROUNDED LARGE LANGUAGE MODELS FOR 3D SCENE REASONING WITHOUT 3D ANNOTATIONS](https://arxiv.org/abs/2506.17545) | RGB-D video / Text | Existing 3D-aware LLMs act as black boxes and rely on pre-trained 3D detectors. Scene-R1 learns to reason about 3D scenes without any point-wise 3D supervision by pairing RL-driven reasoning with a two-stage grounding pipeline: (1) temporal grounding selects relevant video snippets; (2) image grounding predicts 2D boxes refined into 3D via zero-shot segmentation. Training needs only task-level 2D boxes or text labels. Scene-R1 surpasses open-vocabulary baselines while providing transparent step-by-step rationales. | ![1767192789246.png](media/image25.png) | ScanRefer / ScanQA / Nr3D / Sr3D |
| SITE | [SITE: BRIDGING TEXT AND IMAGE MODALITIES WITH LLMS FOR 3D SCENE UNDERSTANDING](https://openreview.net/pdf/fa6e3cb48582a19b228a5cbc3373874931a9e970.pdf) | Single image + text / Multi-view optional | SITE proposes single-image & text encoders for structured scene parsing: (i) Scene2Text extracts instance-level relations; (ii) multi-view observations are transformed into BEV images for spatial relation interpretation; (iii) 1D text and 2D image encoders are fused into LLM fine-tuning for consistent 3D understanding. InPlan3D long-sequence planning benchmark is introduced. SITE shows effectiveness and lower token cost on multiple 3D datasets. | ![1767192956398.png](media/image26.png) | ScanNet / 3RScan / InPlan3D |
| SpaceVista-7B | [SPACEVISTA: CALL-SCALE VISUAL SPATIAL REASONING FROM mm TO km](https://arxiv.org/abs/2510.09606) | RGB video / Text | SpaceVista targets all-scale spatial reasoning (mm ‚Üí km) for robotics & autonomous driving. A specialist-driven pipeline curates SpaceVista-1M (‚âà1M QA pairs, 38K video scenes, 5 scales). SpaceVista-7B uses scale-aware experts and progressive rewards to mitigate knowledge conflict. Evaluations on 5 benchmarks including SpaceVista-Bench show competitive performance and strong generalization across scales and scenarios. | ![1767193247225.png](media/image28.png) | SpaceVista-1M / SpaceVista-Bench / VSI-Bench / SQA3D / SPBench |
| SpatialLM | [SpatialLM: Training Large Language Models for Structured Indoor Modeling](https://arxiv.org/abs/2506.07491) | 3D point cloud / Text | SpatialLM is an LLM that consumes point clouds and outputs structured 3D scene elements (walls, doors, windows, oriented object boxes). Following standard multimodal LLM architecture, it is fine-tuned from open-source LLMs on 54 778 room-scale synthetic point clouds with ground-truth annotations. SpatialLM achieves SOTA on layout estimation and competitive results on 3D object detection, demonstrating a feasible path to enhance LLM spatial understanding for AR/robotics. | ![1767192956398.png](media/image29.png) | Structure3D / S3DIS / ScanNet |
| ViewSpatial-Bench | [ViewSpatial-Bench: Evaluating Multi-perspective Spatial Localization in Vision-Language Models](https://arxiv.org/abs/2505.21500) | Multi-view RGB / Text | Current VLMs perform well on egocentric reasoning but fail under allocentric viewpoints. ViewSpatial-Bench is the first comprehensive benchmark for multi-perspective spatial localization, spanning 5 task types with an automated 3D annotation pipeline. Fine-tuning VLMs on the accompanying multi-perspective dataset yields 46.24% overall improvement, highlighting the importance of explicit 3D spatial relationship modeling. | ![1767194089191.png](media/image32.png) | ViewSpatial-Bench (5 task types) |
| W2R2 | [WHERE, NOT WHAT: COMPELLING VIDEO LLMS TO LEARN GEOMETRIC CAUSALITY FOR 3D-GROUNDING](https://arxiv.org/abs/2505.21500) | RGB-D / Text | VLMs suffer from ‚Äú2D semantic bias‚Äù for coarse 3D localization. W2R2 (What-Where Representation Re-Forming) disentangles representation learning: 2D features act as semantic beacons for ‚ÄúWhat‚Äù, 3D features as spatial anchors for ‚ÄúWhere‚Äù, with dual-objective loss (Alignment + Pseudo-Label) to suppress 2D shortcuts. Experiments on ScanRefer & ScanQA show significant gains in localization accuracy and robustness, especially in cluttered outdoor scenes. | ![1767194157824.png](media/image33.png) | ScanRefer / ScanQA |
| SR-3D | [SR-3D: Spatial Region 3D Aware Vision-Language Model](https://arxiv.org/abs/2509.13317) | RGB (single or multi-view) / Text / 2D box or mask | SR-3D connects single-view 2D images and multi-view 3D data through a shared visual token space, supporting flexible region prompting (2D box, segmentation mask, or direct 3D) without exhaustive multi-frame labeling. 2D features are enriched with 3D positional embeddings, allowing accurate spatial reasoning even when objects do not co-occur in the same view. SR-3D achieves SOTA on both general 2D VL and specialized 3D spatial benchmarks and works on in-the-wild videos without sensory 3D inputs. | ![1767191889929.png](media/image34.png) | VSI-Bench / SQA3D / SPBench / Ref-ScanNet |
