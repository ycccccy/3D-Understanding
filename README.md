<div align="center">
    <h1 style="display: inline-flex; align-items: center;">
        Awesome LLMs for 3D Scene Understanding
    </h1>
</div>
This document summarizes cutting-edge research on 3D scene understanding and foundation models, covering innovative frameworks, datasets, and benchmarks for enhanced spatial reasoning. This document will be continuously updated.


## üìñ News
**[2026/01/04]** Release 3D Scene understanding and 3D foundation model methods.

## üåü Overview

- [üìñ 3D Scene Understanding](#-tasks)
- [üåü 3D Foundation Model](#-overview)



| Method (Pub.) | Paper | Input | Abstract | Framework | Data | GitHub |
|---------------|-------|-------|----------|-----------|------|--------|
| **S¬≤-MLLM (2025-12)** | [S¬≤-MLLM: Boosting Spatial Reasoning Capability of MLLMs for 3D Visual Grounding with Structural Guidance](https://arxiv.org/pdf/2512.01223) | Multi-view RGB / Text | 3D Visual Grounding (3DVG) aims to locate objects in 3D scenes from natural language. MLLMs excel at 2D inputs but struggle with 3D spatial structure. S¬≤-MLLM introduces implicit spatial reasoning: it acquires 3D structural understanding during training via a structure-enhanced module with intra-/inter-view attention and multi-level position encoding, eliminating inefficient point-cloud rendering. Experiments on ScanRefer, Nr3D, Sr3D show superior performance, generalization and efficiency. | ‚Äî | ScanRefer / Nr3D / Sr3D | ‚Äî |
| **SpaceDrive (2025-12)** | [SpaceDrive: Infusing Spatial Awareness into VLM-based Autonomous Driving](https://arxiv.org/abs/2512.10719v1) | Multi-view RGB / Depth / Ego-state / Text | End-to-end autonomous driving methods built on vision language models (VLMs) have undergone rapid development driven by their universal visual understanding and strong reasoning capabilities obtained from the large-scale pretraining. However, we find that current VLMs struggle to understand fine-grained 3D spatial relationships which is a fundamental requirement for systems interacting with the physical world. To address this issue, we propose SpaceDrive, a spatial-aware VLM-based driving framework that treats spatial information as explicit positional encodings (PEs) instead of textual digit tokens, enabling joint reasoning over semantic and spatial representations. SpaceDrive employs a universal positional encoder to all 3D coordinates derived from multi-view depth estimation, historical ego-states, and text prompts. These 3D PEs are first superimposed to augment the corresponding 2D visual tokens. Meanwhile, they serve as a task-agnostic coordinate representation, replacing the digit-wise numerical tokens as both inputs and outputs for the VLM. This mechanism enables the model to better index specific visual semantics in spatial reasoning and directly regress trajectory coordinates rather than generating digit-by-digit, thereby enhancing planning accuracy. Extensive experiments validate that SpaceDrive achieves state-of-the-art open-loop performance on the nuScenes dataset and the second-best Driving Score of 78.02 on the Bench2Drive closed-loop benchmark over existing VLM-based methods. | ‚Äî | nuScenes / Bench2Drive | https://zhenghao2519.github.io/SpaceDrive_Page/ |
| **DSR (2025-12)** | [Learning to Reason in 4D: Dynamic Spatial Reasoning for Vision-Language Models](https://arxiv.org/abs/2512.20557) | RGB video / Text | VLMs excel at static scenes but struggle with dynamic spatial reasoning (DSR) about evolving 3D geometry over time due to scarce 4D-aware data. DSR Suite is proposed: an automated pipeline generates million-scale multiple-choice QA pairs from in-the-wild videos with geometric cues (camera pose, point clouds, trajectories, etc.). A lightweight Geometry Selection Module (GSM) injects question-relevant 4D priors into VLM. Qwen2.5-VL-7B + DSR significantly boosts dynamic spatial reasoning while maintaining general video understanding. | ‚Äî | DSR-Train / DSR-Bench / Kubric-4D / Something-Something V2 | ‚Äî |
| **VLM¬≤ (2025-11)** | [Vision-Language Model with Memory for Spatial Reasoning in 3D Found](https://arxiv.org/abs/2511.20644) | RGB video / Text | Current VLMs fall short in long-horizon video spatial reasoning due to semantic-geometric misalignment and lack of persistent 3D memory. VLM¬≤ introduces a dual-memory module: working memory (sliding window) and episodic memory (long-term consolidation) to maintain a view-consistent 3D-aware representation purely from 2D video. It sets state-of-the-art among video-only models on multiple benchmarks. | ‚Äî | VSI-Bench / SQA3D / SPBench | ‚Äî |
| **SpaceMind (2025-11)** | [SpaceMind: Camera-Guided Modality Fusion for Spatial Reasoning in Vision-Language Models](https://arxiv.org/abs/2511.23075) | RGB / Camera parameters / Text | VLMs still struggle with distance, size comparison and cross-view consistency. SpaceMind adopts a dual-encoder architecture (InternViT + VGGT) and a lightweight Camera-Guided Modality Fusion module that treats camera representation as an active guiding modality rather than passive metadata. The module applies camera-conditioned biasing and query-independent weighting. SpaceMind sets new SOTA on VSI-Bench, SQA3D and SPBench, demonstrating effective spatially grounded intelligence. | ‚Äî | VSI-Bench / SQA3D / SPBench | ‚Äî |
| **3DThinker (2025-10)** | [Think with 3D: Geometric Imagination Grounded Spatial Reasoning from Limited Views](https://arxiv.org/abs/2510.18632) | RGB (single or few views) / Text | Though recent advances in vision‚Äìlanguage models (VLMs) have achieved remarkable progress across a wide range of multimodal tasks, understanding 3D spatial relationships from limited views remains a significant challenge. Previous reasoning methods typically rely on pure text (e.g., topological cognitive maps) or on 2D visual cues. However, their limited representational capacity hinders performance in specific tasks that require 3D spatial imagination. To address this limitation, we propose 3DThinker, a framework that can effectively exploits the rich geometric information embedded within images while reasoning, like humans do. Our framework is the first to enable 3D mentaling during reasoning without any 3D prior input, and it does not rely on explicitly labeled 3D data for training Specifically, our training consists of two stages. First, we perform supervised training to align the 3D latent generated by VLM while reasoning with that of a 3D foundation model (e.g., VGGT). Then, we optimize the entire reasoning trajectory solely based on outcome signals, thereby refining the underlying 3D mentaling. Extensive experiments across multiple benchmarks show that 3DThinker consistently outperforms strong baselines and offers a new perspective toward unifying 3D representations into multimodal reasoning. | ‚Äî | MindCube-Tiny / Ego3D-Bench / VSI-Bench /  SPBench/CV-Bench/SPAR-Bench/ViewSpatialBench
MMSI-Bench | https://github.com/zhangquanchen/3DThinker |
| **SpaceVista-7B (2025-10)** | [SPACEVISTA: CALL-SCALE VISUAL SPATIAL REASONING FROM mm TO km](https://arxiv.org/abs/2510.09606) | RGB video / Text | SpaceVista targets all-scale spatial reasoning (mm ‚Üí km) for robotics & autonomous driving. A specialist-driven pipeline curates SpaceVista-1M (‚âà1M QA pairs, 38K video scenes, 5 scales). SpaceVista-7B uses scale-aware experts and progressive rewards to mitigate knowledge conflict. Evaluations on 5 benchmarks including SpaceVista-Bench show competitive performance and strong generalization across scales and scenarios. | ‚Äî | SpaceVista-1M / SpaceVista-Bench / VSI-Bench / SQA3D / SPBench | ‚Äî |
| **SR-3D (2025-09)** | [SR-3D: Spatial Region 3D Aware Vision-Language Model](https://arxiv.org/abs/2509.13317) | RGB (single or multi-view) / Text / 2D box or mask | SR-3D connects single-view 2D images and multi-view 3D data through a shared visual token space, supporting flexible region prompting (2D box, segmentation mask, or direct 3D) without exhaustive multi-frame labeling. 2D features are enriched with 3D positional embeddings, allowing accurate spatial reasoning even when objects do not co-occur in the same view. SR-3D achieves SOTA on both general 2D VL and specialized 3D spatial benchmarks and works on in-the-wild videos without sensory 3D inputs. | ‚Äî | VSI-Bench / SQA3D / SPBench / Ref-ScanNet | ‚Äî |
| **SD-VLM (2025-09)** | [SD-VLM: Spatial Measuring and Understanding with Depth-Encoded Vision-Language Models](https://arxiv.org/abs/2509.17664) | RGB / Depth / Text | VLMs excel at semantic understanding but lag in quantitative 3D spatial reasoning. SD-VLM introduces (1) MSMU dataset: 700K QA pairs, 2.5M physical annotations, 10K CoT; (2) simple depth positional encoding to boost VLM spatial awareness. SD-VLM achieves SOTA on MSMU-Bench and shows strong generalization on Q-Spatial and SpatialRGPT Bench, outperforming GPT-4o and Intern-VL3-78B by large margins. | ‚Äî | MSMU / MSMU-Bench / Q-Spatial / SpatialRGPT Bench | [https://github.com/cpystan/SD-VLM](https://github.com/cpystan/SD-VLM) |
| **RynnEC (2025-08)** | [RynnEC: Bringing MLLMs into Embodied World](https://arxiv.org/abs/2508.14160) | Egocentric video / Text | RynnEC is a video MLLM for embodied cognition. A compact region encoder + mask decoder enables flexible region-level video interaction. Despite its small size it achieves SOTA on object property understanding, segmentation and spatial reasoning. An egocentric video pipeline generates embodied cognition data; RynnEC-Bench is proposed for evaluation. Code & model are released. | ‚Äî | RynnEC-Bench / Ego4D / Epic-Kitchens | [https://github.com/alibaba-damo-academy/RynnEC](https://github.com/alibaba-damo-academy/RynnEC) |
| **3D-R1 (2025-07)** | [3D-R1: Enhancing Reasoning in 3D VLMs for Unified Scene Understanding](https://arxiv.org/abs/2507.23478) | RGB-D / Text | Current 3D VLMs struggle with robust reasoning due to limited high-quality spatial data and static viewpoint assumptions. 3D-R1 is a foundation model that first constructs Scene-30K, a synthetic CoT dataset via Gemini-2.5-Pro, then applies RLHF (GRPO) with three rewards (perception, semantic, format) to enhance reasoning. A dynamic view-selection strategy adaptively chooses informative perspectives. 3D-R1 improves average performance by 10% across diverse 3D scene benchmarks. | ‚Äî | Scene-30K / ScanQA / SQA3D / ScanRefer / Multi3DRef | ‚Äî |
| **RoboRefer (2025-06)** | [RoboRefer: Towards Spatial Referring with Reasoning in Vision-Language Models for Robotics](https://arxiv.org/pdf/2506.04308) | RGB-D / Text | Spatial referring is crucial for embodied robots, yet VLMs cannot accurately understand complex 3D scenes and reason about interaction locations. RoboRefer is a 3D-aware VLM: (1) supervised fine-tuning integrates a disentangled depth encoder for precise spatial understanding; (2) reinforcement fine-tuning with metric-sensitive process rewards enables multi-step spatial reasoning. RefSpatial dataset (20M QA, 31 relations) and RefSpatial-Bench are introduced. SFT achieves 89.6% success; RFT surpasses Gemini-2.5-Pro by 12.4%. | ‚Äî | RefSpatial / RefSpatial-Bench | ‚Äî |
| **3DRS (2025-06)** | [MLLMs Need 3D-Aware Representation Supervision for Scene Understanding](https://arxiv.org/abs/2506.01946) | Images / Text / 3D Features | Recent advances in scene understanding have leveraged multimodal large language models (MLLMs) for 3D reasoning by capitalizing on their strong 2D pretraining. However, the lack of explicit 3D data during MLLM pretraining limits 3D representation capability. In this paper, we investigate the 3D-awareness of MLLMs by evaluating multi-view correspondence and reveal a strong positive correlation between the quality of 3D-aware representation and downstream task performance. Motivated by this, we propose 3DRS, a framework that enhances MLLM 3D Representation learning through Supervision from pretrained 3D foundation models. Our approach aligns MLLM visual features with rich 3D knowledge distilled from 3D models, effectively improving scene understanding. Extensive experiments across multiple benchmarks and MLLMs‚Äîincluding visual grounding, captioning, and question answering‚Äîdemonstrate consistent performance gains. | ‚Äî | Multi3DRef / ScanRefer3D / Scan2Cap / ScanQA / SQA3D | https://visual-ai.github.io/3drs |
| **Scene-R1 (2025-06)** | [Scene-R1: VIDEO-GROUNDED LARGE LANGUAGE MODELS FOR 3D SCENE REASONING WITHOUT 3D ANNOTATIONS](https://arxiv.org/abs/2506.17545) | RGB-D video / Text | Existing 3D-aware LLMs act as black boxes and rely on pre-trained 3D detectors. Scene-R1 learns to reason about 3D scenes without any point-wise 3D supervision by pairing RL-driven reasoning with a two-stage grounding pipeline: (1) temporal grounding selects relevant video snippets; (2) image grounding predicts 2D boxes refined into 3D via zero-shot segmentation. Training needs only task-level 2D boxes or text labels. Scene-R1 surpasses open-vocabulary baselines while providing transparent step-by-step rationales. | ‚Äî | ScanRefer / ScanQA / Nr3D / Sr3D | ‚Äî |
| **SpatialLM (2025-06)** | [SpatialLM: Training Large Language Models for Structured Indoor Modeling](https://arxiv.org/abs/2506.07491) | 3D point cloud / Text | SpatialLM is an LLM that consumes point clouds and outputs structured 3D scene elements (walls, doors, windows, oriented object boxes). Following standard multimodal LLM architecture, it is fine-tuned from open-source LLMs on 54 778 room-scale synthetic point clouds with ground-truth annotations. SpatialLM achieves SOTA on layout estimation and competitive results on 3D object detection, demonstrating a feasible path to enhance LLM spatial understanding for AR/robotics. | ‚Äî | Structure3D / S3DIS / ScanNet | ‚Äî |
| **SSR (2025-05)** | [SSR: Enhancing Depth Perception in Vision-Language Models via Rational-e-Guided Spatial Reasoning](https://arxiv.org/abs/2505.12448) | RGB / Depth | Despite impressive advancements in Visual-Language Models (VLMs) for multi-modal tasks, their reliance on RGB inputs limits precise spatial understanding. Existing methods for integrating spatial cues either require specialized sensors or fail to effectively exploit depth information. We propose SSR, a framework that transforms raw depth into structured textual rationales and distills them into compact latent embeddings for plug-and-play VLM integration without retraining. We also introduce the million-scale SSR-CoT dataset and SSRBench benchmark. Experiments show SSR substantially improves depth utilization and spatial reasoning. | ‚Äî | SSR-CoT / ScanRefer / Nr3D / Sr3D | https://yliu-cs.github.io/SSR |
| **Spatial-MLLM (2025-05)** | [Spatial-MLLM: Boosting MLLM Capabilities in Visual-based Spatial Intelligence from Pure 2D Observations](https://arxiv.org/abs/2505.23747) | RGB (images or video) / Text | Existing 3D MLLMs rely on extra 3D/2.5D inputs, limiting 2D-only scenarios. Spatial-MLLM unleashes structure priors from a feed-forward visual-geometry foundation model. A dual-encoder extracts semantic (2D) and structure (3D) features; a space-aware frame sampling strategy selects spatially informative frames. Trained on Spatial-MLLM-120k, it achieves SOTA on wide real-world datasets for visual spatial understanding. | ‚Äî | Spatial-MLLM-120k / VSI-Bench / SQA3D / SPBench / Q-Spatial | ‚Äî |
| **SAMA (2025-05)** | [SAMA: Towards Multi-Turn Referential Grounded Video Chat with Large Language Models](https://arxiv.org/abs/2505.18812) | RGB video / Text | Fine-grained spatio-temporal understanding in videos requires jointly mastering video referring (semantics) and video grounding (segmentation). SAMA contributes: (1) SAMA-239K dataset for joint learning; (2) SAMA model with spatio-temporal context aggregator + SAM segmentation head; (3) SAMA-Bench (5 067 Q from 522 videos). SAMA sets new SOTA on SAMA-Bench and general grounding benchmarks while maintaining competitive video understanding performance. | ‚Äî | SAMA-239K / SAMA-Bench / Ref-Youtube-VOS / Ref-DAVIS17 | ‚Äî |
| **ViewSpatial-Bench (2025-05)** | [ViewSpatial-Bench: Evaluating Multi-perspective Spatial Localization in Vision-Language Models](https://arxiv.org/abs/2505.21500) | Multi-view RGB / Text | Current VLMs perform well on egocentric reasoning but fail under allocentric viewpoints. ViewSpatial-Bench is the first comprehensive benchmark for multi-perspective spatial localization, spanning 5 task types with an automated 3D annotation pipeline. Fine-tuning VLMs on the accompanying multi-perspective dataset yields 46.24% overall improvement, highlighting the importance of explicit 3D spatial relationship modeling. | ‚Äî | ViewSpatial-Bench (5 task types) | ‚Äî |
| **W2R2 (2025-05)** | [WHERE, NOT WHAT: COMPELLING VIDEO LLMS TO LEARN GEOMETRIC CAUSALITY FOR 3D-GROUNDING](https://arxiv.org/abs/2505.21500) | RGB-D / Text | VLMs suffer from ‚Äú2D semantic bias‚Äù for coarse 3D localization. W2R2 (What-Where Representation Re-Forming) disentangles representation learning: 2D features act as semantic beacons for ‚ÄúWhat‚Äù, 3D features as spatial anchors for ‚ÄúWhere‚Äù, with dual-objective loss (Alignment + Pseudo-Label) to suppress 2D shortcuts. Experiments on ScanRefer & ScanQA show significant gains in localization accuracy and robustness, especially in cluttered outdoor scenes. | ‚Äî | ScanRefer / ScanQA | ‚Äî |
| **Ross3D (2025-04)** | [Ross3D: Reconstructive Visual Instruction Tuning with 3D-Awareness](https://arxiv.org/abs/2504.01901) | Multi-view RGB / Text | Adapting 2D LMMs to 3D scenes usually relies on designing 3D input-level representations. Ross3D proposes reconstructive visual instruction tuning: training objectives are cross-view reconstruction (recover masked views) and global-view reconstruction (recover Bird‚Äôs-Eye-View images), injecting 3D-aware supervision without extra sensors. Semi-supervised experiments show significant potential to leverage large-scale unlabeled 3D vision-only data while achieving SOTA on multiple 3D scene understanding benchmarks. | ‚Äî | ScanRefer / ScanQA / SQA3D / Multi3DRef | ‚Äî |
